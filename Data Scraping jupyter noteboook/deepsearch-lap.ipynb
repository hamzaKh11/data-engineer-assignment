{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3c7f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# install libraries\n",
    "!pip install \"pymongo[srv]\"\n",
    "!pip install selenium\n",
    "!pip install urllib\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04d106a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df2816b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials to access to database in mongodb\n",
    "uri = \"MONGODB_Link\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "# client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "client = MongoClient(\n",
    "    uri,\n",
    "    tls=True,\n",
    "    tlsAllowInvalidCertificates=True,  # Use this only for testing\n",
    "    socketTimeoutMS=30000,\n",
    "    connectTimeoutMS=30000\n",
    ")\n",
    "\n",
    "# Initialize MongoDB client and connect to the database\n",
    "db = client['bbc_articles']\n",
    "collection = db['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168d657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a new Firefox browser instance using Selenium WebDriver\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Define today's date in the format '2024-05-24'\n",
    "today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Navigate to the BBC website\n",
    "driver.get(\"https://www.bbc.com/\")\n",
    "\n",
    "# Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "# 'html.parser' is specified as the parser to use\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the navbar element\n",
    "navbar_menu = soup.find(\"nav\", {\"class\": \"sc-df0290d6-9 ePpHOZ\"})\n",
    "\n",
    "# Initialize lists to store element names and their corresponding links\n",
    "menu_names = []\n",
    "menu_links = []\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "article_data = []\n",
    "\n",
    "# Iterate over the elements inside the navbar and extract their names and links\n",
    "for item in navbar_menu.find_all(\"a\"):\n",
    "    menu_name = item.text.strip()\n",
    "    menu_link = urljoin(driver.current_url, item.get(\"href\"))  # Use driver.current_url to get the current URL\n",
    "    menu_names.append(menu_name)\n",
    "    menu_links.append(menu_link)\n",
    "\n",
    "# Selecting a subset of menu names and links\n",
    "menu_names = menu_names[1:8]\n",
    "menu_links = menu_links[1:8]\n",
    "\n",
    "# Iterate over each menu link\n",
    "for menu_name, menu_link in zip(menu_names, menu_links):\n",
    "    \n",
    "    # Navigate to each menu in the BBC website\n",
    "    driver.get(menu_link)\n",
    "    # Add a short delay to allow the page to fully load \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # --------------------------------------- Articles related to sport --------------------------------------- #\n",
    "    if menu_name == 'Sport':\n",
    "        \n",
    "        print(\"---------------------------------------------------------------------------------\\n\")\n",
    "        print(f\"------------------------ Menu Name : {menu_name} -------------------------------\\n\")\n",
    "        \n",
    "        # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "        menu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        navbar_submenu = menu_soup.find(\"div\", {\"class\": \"ssrcss-1h87eia-MenuListContainer e14xdrat2\"})\n",
    "\n",
    "        if navbar_submenu:\n",
    "\n",
    "            # Iterate over the elements inside the submenu and extract their names and links\n",
    "            for item in navbar_submenu.find_all(\"a\"):\n",
    "                submenu_name = item.text.strip()\n",
    "                submenu_link = urljoin(driver.current_url, item.get(\"href\")) \n",
    "                \n",
    "                if submenu_name == 'Home':\n",
    "                    \n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"------------------ Submenu Name : {submenu_name} ------------------------------\\n\")\n",
    "                    # Navigate to each submenu in the BBC website\n",
    "                    driver.get(submenu_link)\n",
    "                    # Add a short delay to allow the page to fully load \n",
    "                    time.sleep(2)  \n",
    "                    \n",
    "                    # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                    submenu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    \n",
    "                    # Get the card articles\n",
    "                    article_cards = submenu_soup.find_all(\"div\", {\"type\" : \"article\"})\n",
    "                    \n",
    "                    # Iterate over each card\n",
    "                    for card in article_cards:\n",
    "                        \n",
    "                        # Get the link of each article\n",
    "                        article_link = urljoin(driver.current_url, card.find(\"a\")[\"href\"])\n",
    "                        print(f\"------------------ Article Link : {article_link} ------------------------------\\n\")\n",
    "                        \n",
    "                        # Navigate to each article in the BBC website \n",
    "                        driver.get(article_link)\n",
    "                        # Add a short delay to allow the page to fully load \n",
    "                        time.sleep(2)  \n",
    "                        \n",
    "                        # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                        article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                        # Find the title of article\n",
    "                        title_element = article_soup.find(\"h1\", {\"id\": \"main-heading\"})\n",
    "                        if title_element is not None:\n",
    "                            title = title_element.text.strip()\n",
    "                        else:\n",
    "                            title = '' \n",
    "                        \n",
    "                        # get subtitle \n",
    "                        subtitle_element = article_soup.find(\"b\", {\"class\": \"ssrcss-1xjjfut-BoldText e5tfeyi3\"})\n",
    "                        if subtitle_element is not None:\n",
    "                            subtitle = subtitle_element.text.strip()\n",
    "                        else:\n",
    "                            subtitle = ''  \n",
    "\n",
    "                        # get date published\n",
    "                        script_tag = article_soup.find(\"script\", type=\"application/ld+json\")\n",
    "                        date_published = None\n",
    "                        if script_tag:\n",
    "                            script_content = script_tag.string\n",
    "                            json_content = json.loads(script_content)\n",
    "                            date_published = json_content.get('datePublished')\n",
    "                            \n",
    "                            # Continue only if the datePublished is today\n",
    "                            if date_published and date_published.startswith(today_date):\n",
    "\n",
    "                                # get the author name\n",
    "                                authors = [author.text.strip() for author in article_soup.find_all(\"div\", {\"class\": \"ssrcss-68pt20-Text-TextContributorName e8mq1e96\"})]\n",
    "\n",
    "                                # Get text of article\n",
    "                                text = ''\n",
    "                                text_container = article_soup.find_all(\"div\", {\"data-component\": \"text-block\"})\n",
    "                                for container in text_container:\n",
    "                                    text_elements = container.find_all(\"p\", {\"class\":\"ssrcss-1q0x1qg-Paragraph e1jhz7w10\"})\n",
    "                                    for element in text_elements:\n",
    "                                        text += element.text.strip() + \" \"\n",
    "\n",
    "                                # Initialize an empty list to store the image links\n",
    "                                image_links = []\n",
    "                                image_blocks = article_soup.find_all(\"div\", {\"data-component\": \"image-block\"})\n",
    "                                for block in image_blocks:\n",
    "                                    img_tag = block.find(\"img\")\n",
    "                                    if img_tag:\n",
    "                                        src = img_tag.get(\"src\")\n",
    "                                        image_links.append(src)\n",
    "\n",
    "                                # Get Video link (I put empty because when i try to watch the video inside those article : )\n",
    "                                # ! (they show a alert that my location is restricted to watch this content) !\n",
    "                                video_links = []\n",
    "\n",
    "                                # Get topics\n",
    "                                topics = []\n",
    "                                topic_tags = article_soup.find_all(\"a\", {\"class\": \"ssrcss-1ef12hb-StyledLink ed0g1kj0\"})\n",
    "                                for topic_tag in topic_tags:\n",
    "                                    topics.append(topic_tag.text)\n",
    "\n",
    "                                # Append the scraped data into article_data\n",
    "                                article_data.append({\n",
    "                                    \"Menu\": menu_name,\n",
    "                                    \"Submenu\": submenu_name,\n",
    "                                    \"Title\": title,\n",
    "                                    \"Subtitle\": subtitle,\n",
    "                                    \"Authors\": authors,\n",
    "                                    \"date_published\": date_published,\n",
    "                                    \"Text\": text,\n",
    "                                    \"Images\": image_links,\n",
    "                                    \"Video\": video_links,\n",
    "                                    \"Topics\": topics,\n",
    "                                })\n",
    "\n",
    "                                print('---------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    \n",
    "    # ------- Process All other articles, Exclude sports articles from processing due to their different format; -------- #\n",
    "    else:\n",
    "        \n",
    "        print('---------------------------------------------------------------------------------\\n')\n",
    "        print(f\"------------------ Menu Name : {menu_name} ------------------------------\\n\")\n",
    "        \n",
    "        # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "        menu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        navbar_submenu = menu_soup.find(\"nav\", {\"class\": \"sc-44f1f005-1 cexzQM\"})\n",
    "\n",
    "        if navbar_submenu:\n",
    "\n",
    "            # Iterate over the elements inside the submenu and extract their names and links\n",
    "            for item in navbar_submenu.find_all(\"a\"):\n",
    "                \n",
    "                # Get the submenu names and link\n",
    "                submenu_name = item.text.strip()\n",
    "                submenu_link = urljoin(driver.current_url, item.get(\"href\")) \n",
    "                print(f\"------------------ Submenu Name : {submenu_name} ------------------------------\\n\")\n",
    "\n",
    "                # Navigate to each submenu in the BBC website\n",
    "                driver.get(submenu_link)\n",
    "                # Add a short delay to allow the page to fully load \n",
    "                time.sleep(2)  \n",
    "\n",
    "                # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                submenu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                # Get the card articles\n",
    "                article_cards = submenu_soup.find_all(\"div\", {\"data-testid\" : \"liverpool-card\"})\n",
    "\n",
    "                # Iterate over each article card\n",
    "                for card in article_cards:\n",
    "\n",
    "                    # Get the Article link\n",
    "                    article_link = urljoin(driver.current_url, card.find(\"a\")[\"href\"])\n",
    "                    print(f\"------------------ Article Link : {article_link} ------------------------------\\n\")\n",
    "\n",
    "\n",
    "                    # Find the title of article\n",
    "                    title_element = card.find(\"h2\", {\"data-testid\" : \"card-headline\"})\n",
    "                    if title_element is not None:\n",
    "                        title = title_element.text.strip()\n",
    "                    else:\n",
    "                        title = '' \n",
    "\n",
    "                    # get subtitle\n",
    "                    subtitle_element = card.find(\"p\", {\"data-testid\" : \"card-description\"})\n",
    "                    if subtitle_element is not None:\n",
    "                        subtitle = subtitle_element.text.strip()\n",
    "                    else:\n",
    "                        subtitle = '' \n",
    "\n",
    "                    # Visit each article link and scrape additional data\n",
    "                    driver.get(article_link)\n",
    "                    # Add a short delay to allow the page to fully load\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                    article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                    # Find the script tag with type application/ld+json\n",
    "                    script_tag = article_soup.find(\"script\", type=\"application/ld+json\")\n",
    "\n",
    "                    if script_tag:\n",
    "                        # Extract the content of the script tag\n",
    "                        script_content = script_tag.string\n",
    "\n",
    "                        # Load the JSON content\n",
    "                        json_content = json.loads(script_content)\n",
    "\n",
    "                        # Extract the datePublished\n",
    "                        date_published = json_content.get('datePublished')\n",
    "                        \n",
    "                        # Continue only if the datePublished is today\n",
    "                        if date_published and date_published.startswith(today_date):\n",
    "\n",
    "                            # Get authors of article\n",
    "                            authors = [author.text.strip() for author in article_soup.find_all(\"span\", {\"data-testid\": \"byline-name\"})]\n",
    "\n",
    "                            # Get text of article\n",
    "                            text = \"\"\n",
    "                            text_elements = article_soup.find_all(\"p\", {\"class\": \"sc-eb7bd5f6-0 fYAfXe\"})\n",
    "                            for element in text_elements[:-1]:\n",
    "                                text += element.text.strip() + \" \"\n",
    "\n",
    "                            # Initialize an empty list to store the image links\n",
    "                            image_links = []\n",
    "\n",
    "                            # Find all div elements with data-component=\"image-block\"\n",
    "                            image_blocks = article_soup.find_all(\"div\", {\"data-component\": \"image-block\"})\n",
    "\n",
    "                            # Iterate over each image block\n",
    "                            for block in image_blocks:\n",
    "                                # Find the img tag inside the image block\n",
    "                                img_tag = block.find(\"img\")\n",
    "                                # Check if img tag is found\n",
    "                                if img_tag:\n",
    "                                    # Get the value of the src attribute\n",
    "                                    src = img_tag.get(\"src\")\n",
    "                                    # Append the src value to the image_links list\n",
    "                                    image_links.append(src)\n",
    "\n",
    "                            # Start of Get video link --------------------------------------------------------------------------\n",
    "                            # Extract the JSON data from the script tag\n",
    "                            script_tag = article_soup.find('script', {'id': '__NEXT_DATA__', 'type': 'application/json'})\n",
    "\n",
    "                            # Initialize a list for video links & article_contents\n",
    "                            video_links = []\n",
    "                            article_contents = []\n",
    "\n",
    "                            # Check if the script_tag was found and process it\n",
    "                            if script_tag is not None:\n",
    "                                try:\n",
    "                                    json_data = json.loads(script_tag.string)\n",
    "\n",
    "                                    # Navigate to the relevant part of the JSON structure\n",
    "                                    try:\n",
    "                                        page_key = next(key for key in json_data['props']['pageProps']['page'] if 'news' in key and 'articles' in key)\n",
    "                                        article_contents = json_data['props']['pageProps']['page'][page_key]['contents']\n",
    "                                    except StopIteration:\n",
    "                                        pass\n",
    "\n",
    "                                    # Extract video links if article_contents is not empty\n",
    "                                    if article_contents:\n",
    "                                        for block in article_contents:\n",
    "                                            if block['type'] == 'video':\n",
    "                                                for sub_block in block['model']['blocks']:\n",
    "                                                    if sub_block['type'] == 'media':\n",
    "                                                        for sub_sub_block in sub_block['model']['blocks']:\n",
    "                                                            if sub_sub_block['type'] == 'mediaMetadata':\n",
    "                                                                video_id = sub_sub_block['model']['id']\n",
    "                                                                video_links.append(f\"https://www.bbc.co.uk/iplayer/episode/{video_id}\")\n",
    "                                except json.JSONDecodeError:\n",
    "                                    pass\n",
    "\n",
    "                            # End of Get video link --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                            # Get topics\n",
    "                            topics = []\n",
    "                            topic_tags = article_soup.find_all(\"a\", {\"class\": \"sc-3df0d64d-0 kMyFYO\"})\n",
    "                            for topic_tag in topic_tags:\n",
    "                                topics.append(topic_tag.text)\n",
    "\n",
    "                            # Store scraped data\n",
    "                            article_data.append({\n",
    "                                \"Menu\": menu_name,\n",
    "                                \"Submenu\": submenu_name,\n",
    "                                \"Title\": title,\n",
    "                                \"Subtitle\": subtitle,\n",
    "                                \"Authors\": authors,\n",
    "                                \"date_published\": date_published,\n",
    "                                \"Text\": text,\n",
    "                                \"Images\": image_links,\n",
    "                                \"Video\": video_links,\n",
    "                                \"Topics\": topics,\n",
    "                            })\n",
    "                            print('---------------------------------------------------------------------------------\\n')\n",
    "\n",
    "\n",
    "                    \n",
    "# Insert the list into the collection\n",
    "result = collection.insert_many(article_data)\n",
    "\n",
    "# Print the number of inserted documents\n",
    "print('Number of records added to DB is :', len(result.inserted_ids))\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe293ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records added to DB is : 535\n"
     ]
    }
   ],
   "source": [
    "# Insert the list into the collection\n",
    "result = collection.insert_many(article_data)\n",
    "\n",
    "# Print the number of inserted documents\n",
    "print('Number of records added to DB is :', len(result.inserted_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f56025b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('6650bfc0f12a784f316f3fd4'), 'Menu': 'News', 'Submenu': 'Israel-Gaza War', 'Title': 'Top UN court orders Israel to stop  Rafah offensive', 'Subtitle': 'In a dramatic move, the court supported a South African request to get Israel to halt its operation.', 'Authors': ['Raffi Berg,'], 'date_published': '2024-05-24T13:47:20.059Z', 'Text': 'The UN\\'s top court, the International Court of Justice (ICJ), has issued a dramatic ruling, ordering Israel to \"immediately halt its military offensive in Rafah\". It acted in support of a South African application last week which sought a number of measures against Israel, accusing it of stepping up what it says is a genocide. Israel has vehemently denied the allegation and signalled it would ignore any order to halt its operation. Ahead of Friday\\'s ruling, a government spokesperson said \"no power on Earth will stop Israel from protecting its citizens and going after Hamas in Gaza\". A Hamas spokesman told the BBC: \"We welcome the decision of the International Court of Justice, which demands that the brutal Zionist entity [Israel] stop its aggression\" in Rafah. Reading the court\\'s ruling on Friday, presiding judge Nawaz Salam said that \"Israel must immediately halt its military offensive, and any other action in the Rafah Governorate\" which could bring about \"the physical destruction\" of the Palestinians - alluding to what constitutes genocide under international law. Israel, he added, must also allow unimpeded access to Gaza to any UN body investigating allegations of genocide. The ruling also reiterated a requirement for Israel to enable \"unhindered provision at scale\" of basic services and humanitarian aid for Gaza. \"The humanitarian situation [in Gaza] is now to be characterised as disastrous,\" the ruling said. Judge Salam also said that the court found it \"deeply troubling\" that Israeli hostages were still being held by Hamas and other armed groups in Gaza, and called for \"their immediate and unconditional release\". Minutes after the ruling was delivered, Israel warplanes carried out a series of air strikes on the Shaboura camp in the centre of Rafah. A local activist at nearby Kuwait Hospital told the BBC that rescue teams in the hospital were unable to reach the site of the raids due to their intensity. Israeli began a long-anticipated offensive in Rafah about three weeks ago, vowing to destroy the remaining Hamas battalions there. It says it believes Israeli hostages are also being held in the town. The UN says more than 800,000 Palestinians have fled from Rafah since the offensive began. About 1.5 million had been sheltering there from the fighting elsewhere in Gaza. The hearing is part of a case brought by South Africa to the ICJ in December, claiming Israel was committing genocide in Gaza. That case is ongoing. Israel began its offensive in Gaza after gunmen from Hamas, the organisation which ruled the territory, attacked Israel on 7 October, killing about 1,200 people and taking 252 others back to Gaza as hostages. At least 35,800 Palestinians have been killed in the war since then, according to Gaza\\'s Hamas-run health ministry. Copyright 2024 BBC. All rights reserved.\\xa0\\xa0The BBC is not responsible for the content of external sites.\\xa0Read about our approach to external linking. ', 'Images': ['https://ichef.bbci.co.uk/news/480/cpsprodpb/b824/live/a351c160-19d4-11ef-80aa-699d54c46324.jpg.webp'], 'Video': [], 'Topics': ['Middle East', 'Israel & the Palestinians', 'Israel-Gaza war', 'Israel', 'Palestinian territories', 'Gaza', 'Hamas', 'International Court of Justice']}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the first two documents from the collection\n",
    "documents = collection.find().limit(1)\n",
    "\n",
    "# Process and print the documents\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
