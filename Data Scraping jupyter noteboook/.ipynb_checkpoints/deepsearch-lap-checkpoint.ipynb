{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c3c7f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo[srv] in c:\\users\\khalid\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from pymongo[srv]) (2.6.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\khalid\\anaconda3\\lib\\site-packages (4.8.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: idna in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement urllib\n",
      "ERROR: No matching distribution found for urllib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\khalid\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "# install libraries\n",
    "!pip install \"pymongo[srv]\"\n",
    "!pip install selenium\n",
    "!pip install urllib\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d106a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df2816b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials to access to database in mongodb\n",
    "uri = \"mongodb+srv://deepsearch-lap:VAJcwWdqSiqhsDY1@cluster0.tvscm39.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Initialize MongoDB client and connect to the database\n",
    "db = client['bbc_articles']\n",
    "collection = db['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168d657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a new Firefox browser instance using Selenium WebDriver\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Navigate to the BBC website\n",
    "driver.get(\"https://www.bbc.com/\")\n",
    "\n",
    "# Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "# 'html.parser' is specified as the parser to use\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the navbar element\n",
    "navbar_menu = soup.find(\"nav\", {\"class\": \"sc-df0290d6-9 ePpHOZ\"})\n",
    "\n",
    "# Initialize lists to store element names and their corresponding links\n",
    "menu_names = []\n",
    "menu_links = []\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "article_data = []\n",
    "\n",
    "# Iterate over the elements inside the navbar and extract their names and links\n",
    "for item in navbar_menu.find_all(\"a\"):\n",
    "    menu_name = item.text.strip()\n",
    "    menu_link = urljoin(driver.current_url, item.get(\"href\"))  # Use driver.current_url to get the current URL\n",
    "    menu_names.append(menu_name)\n",
    "    menu_links.append(menu_link)\n",
    "\n",
    "# Selecting a subset of menu names and links\n",
    "menu_names = menu_names[1:8]\n",
    "menu_links = menu_links[1:8]\n",
    "\n",
    "# Iterate over each menu link\n",
    "for menu_name, menu_link in zip(menu_names, menu_links):\n",
    "    \n",
    "    # Navigate to each menu in the BBC website\n",
    "    driver.get(menu_link)\n",
    "    # Add a short delay to allow the page to fully load \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # --------------------------------------- Articles related to sport --------------------------------------- #\n",
    "    if menu_name == 'Sport':\n",
    "        \n",
    "        print(\"---------------------------------------------------------------------------------\\n\")\n",
    "        print(f\"------------------------ Menu Name : {menu_name} -------------------------------\\n\")\n",
    "        \n",
    "        # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "        menu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        navbar_submenu = menu_soup.find(\"div\", {\"class\": \"ssrcss-1h87eia-MenuListContainer e14xdrat2\"})\n",
    "\n",
    "        if navbar_submenu:\n",
    "\n",
    "            # Iterate over the elements inside the submenu and extract their names and links\n",
    "            for item in navbar_submenu.find_all(\"a\"):\n",
    "                submenu_name = item.text.strip()\n",
    "                submenu_link = urljoin(driver.current_url, item.get(\"href\")) \n",
    "                \n",
    "                if submenu_name == 'Home':\n",
    "                    \n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    print(f\"------------------ Submenu Name : {submenu_name} ------------------------------\\n\")\n",
    "                    # Navigate to each submenu in the BBC website\n",
    "                    driver.get(submenu_link)\n",
    "                    # Add a short delay to allow the page to fully load \n",
    "                    time.sleep(2)  \n",
    "                    \n",
    "                    # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                    submenu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    \n",
    "                    # Get the card articles\n",
    "                    article_cards = submenu_soup.find_all(\"div\", {\"type\" : \"article\"})\n",
    "                    \n",
    "                    # Iterate over each card\n",
    "                    for card in article_cards:\n",
    "                        \n",
    "                        # Get the link of each article\n",
    "                        article_link = urljoin(driver.current_url, card.find(\"a\")[\"href\"])\n",
    "                        print(f\"------------------ Article Link : {article_link} ------------------------------\\n\")\n",
    "                        \n",
    "                        # Navigate to each article in the BBC website \n",
    "                        driver.get(article_link)\n",
    "                        # Add a short delay to allow the page to fully load \n",
    "                        time.sleep(2)  \n",
    "                        \n",
    "                        # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                        article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                        # Find the title of article\n",
    "                        title_element = article_soup.find(\"h1\", {\"id\": \"main-heading\"})\n",
    "                        if title_element is not None:\n",
    "                            title = title_element.text.strip()\n",
    "                        else:\n",
    "                            title = '' \n",
    "                        \n",
    "                        # get subtitle \n",
    "                        subtitle_element = article_soup.find(\"b\", {\"class\": \"ssrcss-1xjjfut-BoldText e5tfeyi3\"})\n",
    "                        if subtitle_element is not None:\n",
    "                            subtitle = subtitle_element.text.strip()\n",
    "                        else:\n",
    "                            subtitle = ''  \n",
    "\n",
    "                        # get date published\n",
    "                        script_tag = article_soup.find(\"script\", type=\"application/ld+json\")\n",
    "                        date_published = None\n",
    "                        if script_tag:\n",
    "                            script_content = script_tag.string\n",
    "                            json_content = json.loads(script_content)\n",
    "                            date_published = json_content.get('datePublished')\n",
    "\n",
    "                        # get the author name\n",
    "                        authors = [author.text.strip() for author in article_soup.find_all(\"div\", {\"class\": \"ssrcss-68pt20-Text-TextContributorName e8mq1e96\"})]\n",
    "\n",
    "                        # Get text of article\n",
    "                        text = ''\n",
    "                        text_container = article_soup.find_all(\"div\", {\"data-component\": \"text-block\"})\n",
    "                        for container in text_container:\n",
    "                            text_elements = container.find_all(\"p\", {\"class\":\"ssrcss-1q0x1qg-Paragraph e1jhz7w10\"})\n",
    "                            for element in text_elements:\n",
    "                                text += element.text.strip() + \" \"\n",
    "\n",
    "                        # Initialize an empty list to store the image links\n",
    "                        image_links = []\n",
    "                        image_blocks = article_soup.find_all(\"div\", {\"data-component\": \"image-block\"})\n",
    "                        for block in image_blocks:\n",
    "                            img_tag = block.find(\"img\")\n",
    "                            if img_tag:\n",
    "                                src = img_tag.get(\"src\")\n",
    "                                image_links.append(src)\n",
    "\n",
    "                        # Get Video link (I put empty because when i try to watch the video inside those article : )\n",
    "                        # ! (they show a alert that my location is restricted to watch this content) !\n",
    "                        video_links = []\n",
    "\n",
    "                        # Get topics\n",
    "                        topics = []\n",
    "                        topic_tags = article_soup.find_all(\"a\", {\"class\": \"ssrcss-1ef12hb-StyledLink ed0g1kj0\"})\n",
    "                        for topic_tag in topic_tags:\n",
    "                            topics.append(topic_tag.text)\n",
    "\n",
    "                        # Append the scraped data into article_data\n",
    "                        article_data.append({\n",
    "                            \"Menu\": menu_name,\n",
    "                            \"Submenu\": submenu_name,\n",
    "                            \"Title\": title,\n",
    "                            \"Subtitle\": subtitle,\n",
    "                            \"Authors\": authors,\n",
    "                            \"date_published\": date_published,\n",
    "                            \"Text\": text,\n",
    "                            \"Images\": image_links,\n",
    "                            \"Video\": video_links,\n",
    "                            \"Topics\": topics,\n",
    "                        })\n",
    "                        \n",
    "                        print('---------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    \n",
    "    # ------- Process All other articles, Exclude sports articles from processing due to their different format; -------- #\n",
    "    else:\n",
    "        \n",
    "        print('---------------------------------------------------------------------------------\\n')\n",
    "        print(f\"------------------ Menu Name : {menu_name} ------------------------------\\n\")\n",
    "        \n",
    "        # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "        menu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        navbar_submenu = menu_soup.find(\"nav\", {\"class\": \"sc-44f1f005-1 cexzQM\"})\n",
    "\n",
    "        if navbar_submenu:\n",
    "\n",
    "            # Iterate over the elements inside the submenu and extract their names and links\n",
    "            for item in navbar_submenu.find_all(\"a\"):\n",
    "                \n",
    "                # Get the submenu names and link\n",
    "                submenu_name = item.text.strip()\n",
    "                submenu_link = urljoin(driver.current_url, item.get(\"href\")) \n",
    "                print(f\"------------------ Submenu Name : {submenu_name} ------------------------------\\n\")\n",
    "\n",
    "                # Navigate to each submenu in the BBC website\n",
    "                driver.get(submenu_link)\n",
    "                # Add a short delay to allow the page to fully load \n",
    "                time.sleep(2)  \n",
    "                \n",
    "                # Retrieve the page source from the browser and parse it using BeautifulSoup\n",
    "                submenu_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                \n",
    "                # Get the card articles\n",
    "                article_cards = submenu_soup.find_all(\"div\", {\"data-testid\": \"liverpool-card\"})\n",
    "                \n",
    "                # Iterate over each article card\n",
    "                for card in article_cards:\n",
    "                    \n",
    "                    # Get the title\n",
    "                    title = card.find(\"h2\").text.strip()\n",
    "                    \n",
    "                    # Get the subtitle\n",
    "                    subtitle = card.find(\"p\").text.strip()\n",
    "                    \n",
    "                    # Get the Article link# Get the card articles\n",
    "                    article_link = urljoin(driver.current_url, card.find(\"a\")[\"href\"])\n",
    "\n",
    "                    print(f\"------------------ Article Link : {article_link} ------------------------------\\n\")\n",
    "\n",
    "                    # Visit each article link and scrape additional data\n",
    "                    driver.get(article_link)\n",
    "                    time.sleep(2)  # Add a short delay to allow the page to fully load (adjust as needed)\n",
    "\n",
    "                    article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                    # Find the script tag with type application/ld+json\n",
    "                    script_tag = article_soup.find(\"script\", type=\"application/ld+json\")\n",
    "\n",
    "                    if script_tag:\n",
    "                        # Extract the content of the script tag\n",
    "                        script_content = script_tag.string\n",
    "\n",
    "                        # Load the JSON content\n",
    "                        json_content = json.loads(script_content)\n",
    "\n",
    "                        # Extract the datePublished\n",
    "                        date_published = json_content.get('datePublished')\n",
    "\n",
    "                    # Get authors of article\n",
    "                    authors = [author.text.strip() for author in article_soup.find_all(\"span\", {\"data-testid\": \"byline-name\"})]\n",
    "\n",
    "                    # Get text of article\n",
    "                    text = \"\"\n",
    "                    text_elements = article_soup.find_all(\"p\", {\"class\": \"sc-eb7bd5f6-0 fYAfXe\"})\n",
    "\n",
    "                    for element in text_elements[:-1]:\n",
    "                        text += element.text.strip() + \" \"\n",
    "\n",
    "\n",
    "                    # Initialize an empty list to store the image links\n",
    "                    image_links = []\n",
    "\n",
    "                    # Find all div elements with data-component=\"image-block\"\n",
    "                    image_blocks = article_soup.find_all(\"div\", {\"data-component\": \"image-block\"})\n",
    "\n",
    "                    # Iterate over each image block\n",
    "                    for block in image_blocks:\n",
    "                        # Find the img tag inside the image block\n",
    "                        img_tag = block.find(\"img\")\n",
    "                        # Check if img tag is found\n",
    "                        if img_tag:\n",
    "                            # Get the value of the src attribute\n",
    "                            src = img_tag.get(\"src\")\n",
    "                            # Append the src value to the image_links list\n",
    "                            image_links.append(src)\n",
    "\n",
    "\n",
    "                    # Start of Get video link --------------------------------------------------------------------------\n",
    "                    # Extract the JSON data from the script tag\n",
    "                    script_tag = article_soup.find('script', {'id': '__NEXT_DATA__', 'type': 'application/json'})\n",
    "\n",
    "                    # Initialize a list for video links & article_contents\n",
    "                    video_links = []\n",
    "                    article_contents = []\n",
    "\n",
    "                    # Check if the script_tag was found and process it\n",
    "                    if script_tag is not None:\n",
    "                        try:\n",
    "                            json_data = json.loads(script_tag.string)\n",
    "\n",
    "                            # Navigate to the relevant part of the JSON structure\n",
    "                            try:\n",
    "                                page_key = next(key for key in json_data['props']['pageProps']['page'] if 'news' in key and 'articles' in key)\n",
    "                                article_contents = json_data['props']['pageProps']['page'][page_key]['contents']\n",
    "                            except StopIteration:\n",
    "                                pass\n",
    "\n",
    "                            # Extract video links if article_contents is not empty\n",
    "                            if article_contents:\n",
    "                                for block in article_contents:\n",
    "                                    if block['type'] == 'video':\n",
    "                                        for sub_block in block['model']['blocks']:\n",
    "                                            if sub_block['type'] == 'media':\n",
    "                                                for sub_sub_block in sub_block['model']['blocks']:\n",
    "                                                    if sub_sub_block['type'] == 'mediaMetadata':\n",
    "                                                        video_id = sub_sub_block['model']['id']\n",
    "                                                        video_links.append(f\"https://www.bbc.co.uk/iplayer/episode/{video_id}\")\n",
    "                        except json.JSONDecodeError:\n",
    "                            pass\n",
    "\n",
    "\n",
    "\n",
    "                    # Extract video links\n",
    "                    for block in article_contents:\n",
    "                        if block['type'] == 'video':\n",
    "                            for sub_block in block['model']['blocks']:\n",
    "                                if sub_block['type'] == 'media':\n",
    "                                    for sub_sub_block in sub_block['model']['blocks']:\n",
    "                                        if sub_sub_block['type'] == 'mediaMetadata':\n",
    "                                            video_id = sub_sub_block['model']['id']\n",
    "                                            video_links.append(f\"https://www.bbc.co.uk/iplayer/episode/{video_id}\")\n",
    "                    # End of Get video link --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    # Get topics\n",
    "                    topics = []\n",
    "                    topic_tags = article_soup.find_all(\"a\", {\"class\": \"sc-3df0d64d-0 kMyFYO\"})\n",
    "                    for topic_tag in topic_tags:\n",
    "                        topics.append(topic_tag.text)\n",
    "\n",
    "                    # Store scraped data\n",
    "                    article_data.append({\n",
    "                        \"Menu\": menu_name,\n",
    "                        \"Submenu\": submenu_name,\n",
    "                        \"Title\": title,\n",
    "                        \"Subtitle\": subtitle,\n",
    "                        \"Authors\": authors,\n",
    "                        \"date_published\": date_published,\n",
    "                        \"Text\": text,\n",
    "                        \"Images\": image_links,\n",
    "                        \"Video\": video_links,\n",
    "                        \"Topics\": topics,\n",
    "                    })\n",
    "                    print('---------------------------------------------------------------------------------\\n')\n",
    "                    \n",
    "# Insert the data into MongoDB\n",
    "collection.insert_one(article_data)\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cf024",
   "metadata": {},
   "source": [
    "# Now let's move to sport submenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d1ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5b276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the article data from the article page !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "driver.get(\"https://www.bbc.com/sport/football/articles/c2llxjxq955o\")\n",
    "time.sleep(2)  # Add a short delay to allow the page to fully load (adjust as needed)\n",
    "\n",
    "article_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the title of article\n",
    "title = article_soup.find(\"h1\", {\"id\": \"main-heading\"}).text.strip()\n",
    "\n",
    "# get subtitle \n",
    "subtitle = article_soup.find(\"b\", {\"class\": \"ssrcss-1xjjfut-BoldText e5tfeyi3\"}).text.strip()\n",
    "\n",
    "# get date published\n",
    "script_tag = article_soup.find(\"script\", type=\"application/ld+json\")\n",
    "date_published = None\n",
    "if script_tag:\n",
    "    script_content = script_tag.string\n",
    "    json_content = json.loads(script_content)\n",
    "    date_published = json_content.get('datePublished')\n",
    "\n",
    "# get the author name\n",
    "authors = [author.text.strip() for author in article_soup.find_all(\"div\", {\"class\": \"ssrcss-68pt20-Text-TextContributorName e8mq1e96\"})]\n",
    "\n",
    "# Get text of article\n",
    "text = \"\"\n",
    "text_container = article_soup.find_all(\"div\", {\"data-component\": \"text-block\"})\n",
    "for container in text_container:\n",
    "    text_elements = container.find_all(\"p\", {\"class\":\"ssrcss-1q0x1qg-Paragraph e1jhz7w10\"})\n",
    "    for element in text_elements:\n",
    "        text += element.text.strip() + \" \"\n",
    "\n",
    "# Initialize an empty list to store the image links\n",
    "image_links = []\n",
    "image_blocks = article_soup.find_all(\"div\", {\"data-component\": \"image-block\"})\n",
    "for block in image_blocks:\n",
    "    img_tag = block.find(\"img\")\n",
    "    if img_tag:\n",
    "        src = img_tag.get(\"src\")\n",
    "        image_links.append(src)\n",
    "\n",
    "# Get Video link\n",
    "video_links = []\n",
    "\n",
    "# Get topics\n",
    "topics = []\n",
    "topic_tags = article_soup.find_all(\"a\", {\"class\": \"ssrcss-1ef12hb-StyledLink ed0g1kj0\"})\n",
    "for topic_tag in topic_tags:\n",
    "    topics.append(topic_tag.text)\n",
    "\n",
    "# Store scraped data\n",
    "article_data.append({\n",
    "    \"Menu\": menu_name,\n",
    "    \"Submenu\": submenu_name,\n",
    "    \"Title\": title,\n",
    "    \"Subtitle\": subtitle,\n",
    "    \"Authors\": authors,\n",
    "    \"date_published\": date_published,\n",
    "    \"Text\": text,\n",
    "    \"Images\": image_links,\n",
    "    \"Video\": video_links,\n",
    "    \"Topics\": topics,\n",
    "})\n",
    "\n",
    "# Insert the data into MongoDB\n",
    "collection.insert_one(article_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
